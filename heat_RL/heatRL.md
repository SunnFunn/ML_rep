### **Диспетчерское управление температурой теплоносителя с помощью RL model**

[Проект](https://github.com/SunnFunn/ML_rep/tree/master/heat_RL) выполнен как modbus server с имитацией modbus client для целей тестирования modbus server.

Приложение разработано для работы в течение отопительного сезона в следующем цикле:
> - Клиент сервер SCADA (в настоящем приложении клиент имитируется модулем pymodbus для целей тестировки приложения) общается с сервером modbus по ip (localhost:5020);
> - Клиент отправляет серверу каждые 3 часа (цикл выбора уставки температуры подающего теплоносителя) температуру внутри помещения (реальный клиент получает ее с датчиков в помещении);
> - Сервер также каждые 3 часа обращается к сайту rp5.ru по метеостанции г. Москва ВДНХ и забирает с него данные о текущей температуре наружного воздуха, скорости ветра, влажности и облачности;
> - Имея текущее состояние (state), включающее t наружного воздуха текущую и через 3 часа (прогноз), t внутри помещения (получена от Клиента), данные о ветре, влажности, облачности, а также целевые t внутри помещения текущую и через 3 часа (17 гр. Цельсия в модели), сервер передает state в policy model и получает предлагаемую моделью t подачи теплоносителя в помещение;
> - state, при этом, записывается в файл опыта expirience.csv в папке data для целей последующего обновления policy model на актуальном опыте;
> - t подачи теплоносителя запоминается в holding registers сервера, а Клиент забирает эти данные для управления системой теплоснабжения (выставляет уставку температуры теплоносителя на ближайшие 3 часа);
> - сервер после получения текущего state запускает функцию обновления policy model и q model, корректируя веса нейросети политики и ценности для учета текущего опыта;
> - далее цикл повторяется каждые 3 часа в течение отопительного сезона.

Ниже представлены два скриншота:
> * первый скриншот сервера показывает данные, которые сервер получил от клиента, с сайта rp5.ru, целевые температуры, рассчитанную им температуру теплоносителя и пишет данные, касающиеся обновления моделей по текущему опыту (losses);
> * второй скриншот показывает данные которые клиент отправил серверу (t внутри помещения) и данные, которые он получил от сервера.

![heatRL_1](https://github.com/SunnFunn/ML_rep/blob/master/imgs/heatRL_1.png)


![heatRL_2](https://github.com/SunnFunn/ML_rep/blob/master/imgs/heatRL_2.png)


Сервер modbus работает в асинхронном режиме, обеспеченном модулем python asyncio.

Тренировка модели:
> - Модель оценки температуры подаваемого в помещение для отопления теплоносителя расчитывается нейронной сетью policy model;
> - Тренировка policy model на реальных текущих данных не возможна "с нуля" в силу малости объема текущих данных, получаемых за один отопительный сезон(5200 часов отопительного сезона на 3 часа цикла составит примерно 1700+ данных);
> - По этой причине модель предварительно натренирована на расчетных по характеристиками отапливаемого здания (зависимость остывания здания от температур воздуха снаружи, внутри и отопительной воды);
> - Policy model и q model тренировались по методу DDPG (deep deterministic policy gradient) файл с кодом тренировки можно найти в папке ./het_RL/app/model/Heat_RLDDPG.ipynb;
> - Предварительно натренированная модель в дальнейшем каждые 3 часа обновляется на реальных данных с учетом реальных характеристик здания в разных климатических условиях.
